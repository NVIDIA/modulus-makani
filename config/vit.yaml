full_field: &BASE_CONFIG

    # metadata file for the dataset
    metadata_json_path: "/metadata/data.json"

    # data 
    train_data_path: "/train"
    valid_data_path: "/test"
    exp_dir: "/runs"

    # files used for normalization of the data
    min_path: "/stats/mins.npy"
    max_path: "/stats/maxs.npy"
    time_means_path:   "/stats/time_means.npy"
    global_means_path: "/stats/global_means.npy"
    global_stds_path:  "/stats/global_stds.npy"
    time_diff_means_path: "/stats/time_diff_means.npy"
    time_diff_stds_path: "/stats/time_diff_stds.npy"

    # training parameters
    loss: "absolute squared geometric l2"
    channel_weights: "auto"
    lr: 5E-4
    n_eval_samples: 8760
    max_epochs: 150
    batch_size: 32
    weight_decay: 0.0

    # scheduler settings
    scheduler: "CosineAnnealingLR" # "ReduceLROnPlateau"
    scheduler_T_max: 150
    lr_warmup_steps: 0

    # dropouts
    path_drop_rate: 0.
    mlp_drop_rate: 0.
    attn_drop_rate: 0.

    # wireup stuff
    wireup_info: "mpi"
    wireup_store: "tcp"

    num_data_workers: 4
    num_visualization_workers: 2
    dt: 1 # how many timesteps ahead the model will predict
    n_history: 0 #how many previous timesteps to consider
    prediction_type: "iterative"
    prediction_length: 35 #applicable only if prediction_type == "iterative"
    n_initial_conditions: 5 #applicable only if prediction_type == "iterative"
    valid_autoreg_steps: 20 # number of autoregressive steps for validation

    ics_type: "specify_number"
    save_raw_forecasts: !!bool True
    save_channel: !!bool False
    masked_acc: !!bool False
    maskpath: None
    perturb: !!bool False
    add_noise: !!bool False
    noise_std: 0.

    # network related
    nettype: "ViT"
    depth: 12
    model_grid_type: "equiangular"
    patch_size: [7,8]
    num_heads: 8
    embed_dim: 384
    normalization_layer: "layer_norm"

    #options default, residual
    target: "default" 

    channel_names: ["u10m", "v10m", "u100m", "v100m", "t2m", "sp", "msl", "tcwv", "u50", "u100", "u150", "u200", "u250", "u300", "u400", "u500", "u600", "u700", "u850", "u925", "u1000", "v50", "v100", "v150", "v200", "v250", "v300", "v400", "v500", "v600", "v700", "v850", "v925", "v1000", "z50", "z100", "z150", "z200", "z250", "z300", "z400", "z500", "z600", "z700", "z850", "z925", "z1000", "t50", "t100", "t150", "t200", "t250", "t300", "t400", "t500", "t600", "t700", "t850", "t925", "t1000", "q50", "q100", "q150", "q200", "q250", "q300", "q400", "q500", "q600", "q700", "q850", "q925", "q1000"]
    normalization: "zscore" #options zscore or minmax
    hard_thresholding_fraction: 1.0
  
    # extra channels
    add_grid: !!bool False
    gridtype: "sinusoidal"
    grid_num_frequencies: 16
    roll: !!bool False
    add_zenith: !!bool True
    # invariants
    add_orography: !!bool True
    orography_path: /invariants/orography.nc
    add_landmask: !!bool True
    landmask_path: /invariants/land_sea_mask.nc


    finetune: !!bool False

    log_to_screen: !!bool True
    log_to_wandb: !!bool True
    log_video: 10 # if > 0 will log every i-th epoch
    save_checkpoint: !!bool True

    optimizer_type: "Adam"
    optimizer_beta1: 0.9
    optimizer_beta2: 0.999
    optimizer_max_grad_norm: 1.0 

    # Weights and biases configuration
    wandb_name: None # If None, config will be used but you can override it here
    wandb_group: None # If None, will be "era5_wind" + config, but you can override it here
    wandb_project: "distributed sfno training"
    wandb_entity: "sfno-large-model-training" # but your username here

    inf_data_path: "/out_of_sample"

vit_73ch: &VIT_73CH
    <<: *BASE_CONFIG

    wandb_group: "vit_73ch"

    max_epochs: 150

vit_73ch_big:
    <<: *BASE_CONFIG

    wandb_group: "vit_73ch"

    embed_dim: 1024
    batch_size: 256
    lr: 1e-3